{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAMBRA AI Agent\n",
    "\n",
    "Based on [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) Reinforcement Learning library\n",
    "\n",
    "Using [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347.pdf) algorithm\n",
    "\n",
    "This agent takes about 6 weeks of 24/7 training on a mid/low level workstation (i5 proc, 16 Gb Ram, 4Gb Nvidia GPU) to reach about 70M steps and a mean cumulative reward of 14. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from diambra_environment.diambraGym import diambraGym\n",
    "from diambra_environment.makeDiambraEnvSB import make_diambra_env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_base_path = os.path.join(os.path.abspath(\"\"), \"../../\") # Absolute path to your DIAMBRA environment\n",
    "\n",
    "diambraEnvKwargs = {}\n",
    "diambraEnvKwargs[\"gameId\"]          = \"doapp\"\n",
    "diambraEnvKwargs[\"roms_path\"]       = os.path.join(repo_base_path, \"roms/\") # Absolute path to roms\n",
    "\n",
    "diambraEnvKwargs[\"mame_diambra_step_ratio\"] = 6\n",
    "diambraEnvKwargs[\"render\"]      = True\n",
    "diambraEnvKwargs[\"lock_fps\"]    = False # Locks to 60 FPS\n",
    "diambraEnvKwargs[\"sound\"]       = diambraEnvKwargs[\"lock_fps\"] and diambraEnvKwargs[\"render\"]\n",
    "\n",
    "# 1P\n",
    "diambraEnvKwargs[\"player\"] = \"Random\"\n",
    "\n",
    "# Game specific\n",
    "diambraEnvKwargs[\"difficulty\"] = 3\n",
    "diambraEnvKwargs[\"characters\"]  = [[\"Kasumi\", \"Random\"], [\"Kasumi\", \"Random\"]]\n",
    "diambraEnvKwargs[\"charOutfits\"] = [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorBoardFolder = \"./{}_ppo2_TB_CustCnn_bL_d_noComb/\".format(diambraEnvKwargs[\"gameId\"])\n",
    "modelFolder = \"./{}_ppo2_Model_CustCnn_bL_d_noComb/\".format(diambraEnvKwargs[\"gameId\"])\n",
    "\n",
    "os.makedirs(modelFolder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from diambra_environment.customPolicies.utils import linear_schedule, AutoSave\n",
    "from diambra_environment.customPolicies.f import *\n",
    "\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAMBRA gym kwargs\n",
    "diambraGymKwargs = {}\n",
    "diambraGymKwargs[\"P2brain\"]               = None\n",
    "diambraGymKwargs[\"continue_game\"]         = 0.0\n",
    "diambraGymKwargs[\"show_final\"]            = False\n",
    "diambraGymKwargs[\"gamePads\"]              = [None, None]\n",
    "diambraGymKwargs[\"actionSpace\"]           = [\"discrete\", \"multiDiscrete\"]\n",
    "diambraGymKwargs[\"attackButCombinations\"] = [False, False]\n",
    "diambraGymKwargs[\"actBufLen\"]             = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrappers kwargs\n",
    "wrapperKwargs = {}\n",
    "wrapperKwargs[\"hwc_obs_resize\"]    = [256, 256, 1]\n",
    "wrapperKwargs[\"normalize_rewards\"] = True\n",
    "wrapperKwargs[\"clip_rewards\"]      = False\n",
    "wrapperKwargs[\"frame_stack\"]       = 6\n",
    "wrapperKwargs[\"dilation\"]          = 1\n",
    "wrapperKwargs[\"scale\"]             = True\n",
    "wrapperKwargs[\"scale_mod\"]         = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Observations\n",
    "keyToAdd = []\n",
    "keyToAdd.append(\"actionsBuf\")\n",
    "keyToAdd.append(\"ownHealth\")\n",
    "keyToAdd.append(\"oppHealth\")\n",
    "keyToAdd.append(\"ownPosition\")\n",
    "keyToAdd.append(\"oppPosition\")\n",
    "keyToAdd.append(\"stage\")\n",
    "keyToAdd.append(\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEnv=8\n",
    "\n",
    "envId = \"Train\"\n",
    "env = make_diambra_env(diambraGym, env_prefix=envId, num_env=numEnv, seed=timeDepSeed, \n",
    "                       diambra_kwargs=diambraEnvKwargs, \n",
    "                       diambra_gym_kwargs=diambraGymKwargs,\n",
    "                       wrapper_kwargs=wrapperKwargs, \n",
    "                       key_to_add=keyToAdd, use_subprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Obs_space = \", env.observation_space)\n",
    "print(\"Obs_space type = \", env.observation_space.dtype)\n",
    "print(\"Obs_space high = \", env.observation_space.high)\n",
    "print(\"Obs_space low = \", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Act_space = \", env.action_space)\n",
    "print(\"Act_space type = \", env.action_space.dtype)\n",
    "if diambraGymKwargs[\"actionSpace\"][0] == \"multiDiscrete\":\n",
    "    print(\"Act_space n = \", env.action_space.nvec)\n",
    "else:\n",
    "    print(\"Act_space n = \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy param\n",
    "\n",
    "n_actions = env.get_attr(\"n_actions\")[0][0]\n",
    "actBufLen = diambraGymKwargs[\"actBufLen\"]\n",
    "\n",
    "policyKwargs={}\n",
    "policyKwargs[\"n_add_info\"] = actBufLen*(n_actions[0]+n_actions[1]) + len(keyToAdd)-2 # No Char Info\n",
    "policyKwargs[\"layers\"] = [64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO param\n",
    "\n",
    "setGamma = 0.94\n",
    "\n",
    "setLearningRate = linear_schedule(2.5e-4, 2.5e-6)\n",
    "#setLearningRate = linear_schedule(5.0e-5, 2.5e-6)\n",
    "\n",
    "setClipRange = linear_schedule(0.15, 0.025)\n",
    "#setClipRange = linear_schedule(0.05, 0.025)\n",
    "\n",
    "setClipRangeVf = setClipRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = PPO2(CustCnnPolicy, env, verbose=1, \n",
    "             gamma=setGamma, nminibatches=4, noptepochs=4, n_steps=128,\n",
    "             learning_rate=setLearningRate, cliprange=setClipRange, \n",
    "             cliprange_vf=setClipRangeVf, \n",
    "             tensorboard_log=tensorBoardFolder, policy_kwargs=policyKwargs)\n",
    "\n",
    "#OR\n",
    "\n",
    "# Load the trained agent\n",
    "#model = PPO2.load(os.path.join(modelFolder, \"20M\"), env=env, tensorboard_log=tensorBoardFolder, \n",
    "#                  policy_kwargs=policyKwargs, gamma = setGamma, learning_rate=setLearningRate, \n",
    "#                  cliprange=setClipRange, cliprange_vf=setClipRangeVf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model discount factor = \", model.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: autosave every USER DEF steps\n",
    "autoSaveCallback = AutoSave(check_freq=1000000, numEnv=numEnv, save_path=modelFolder+\"0M_\")\n",
    "\n",
    "# Train the agent\n",
    "time_steps = 20000000\n",
    "model.learn(total_timesteps=time_steps, callback=autoSaveCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(os.path.join(modelFolder, \"20M\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new evaluation environment\n",
    "diambraEnvKwargs[\"render\"] = True\n",
    "\n",
    "envId = \"Test\"\n",
    "env = make_diambra_env(diambraGym, env_prefix=envId, num_env=1, seed=timeDepSeed, \n",
    "                       diambra_kwargs=diambraEnvKwargs, diambra_gym_kwargs=diambraGymKwargs,\n",
    "                       wrapper_kwargs=wrapperKwargs, key_to_add=keyToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "cumulativeEpRew = 0.0\n",
    "cumulativeEpRewAll = []\n",
    "\n",
    "maxNumEp = 10\n",
    "currNumEp = 0\n",
    "\n",
    "while currNumEp < maxNumEp:\n",
    "\n",
    "    action = model.predict(observation, deterministic=True)\n",
    "    #action_prob = model.action_probability(observation, states)\n",
    "    #print(\"Action probabilities = \", action_prob)\n",
    "    #print(\"Max action = \", np.argmax(action_prob))\n",
    "    #print(\"Action = \", action)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action[0])\n",
    "    \n",
    "    cumulativeEpRew += reward\n",
    "    \n",
    "    if np.any(done):\n",
    "        currNumEp += 1\n",
    "        print(\"Ep. # = \", currNumEp)\n",
    "        print(\"Ep. Cumulative Rew # = \", cumulativeEpRew)\n",
    "        cumulativeEpRewAll.append(cumulativeEpRew)\n",
    "        cumulativeTotRew += cumulativeEpRew\n",
    "        cumulativeEpRew = 0.0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cumulative reward = \", cumulativeEpRewAll)    \n",
    "print(\"Mean cumulative reward = \", np.mean(cumulativeEpRewAll))    \n",
    "print(\"Std cumulative reward = \", np.std(cumulativeEpRewAll))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
